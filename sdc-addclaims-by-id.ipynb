{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9179ac-ef57-4698-8b48-1c3ed25a046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bot to examine Wikimedia Commons category and add structured data (SDC) info for The Met Museum:\n",
    "#   Met Object ID (P3634) -> object id\n",
    "#   collection (P195) -> Q160236\n",
    "#\n",
    "# For well-structured uploads to Commons (mostly done in 2017 with GLAM Wiki Toolset)\n",
    "#   the object id can be determined by looking at the template \"source\" field\n",
    "#   for a URL of pattern:\n",
    "#   https://www.metmuseum.org/art/collection/search/12345\n",
    "#\n",
    "# Most of the institutional uploads located in: \n",
    "#   https://commons.wikimedia.org/wiki/Category:Metropolitan_Museum_of_Art_by_department\n",
    "#\n",
    "# Author: Andrew Lih (User:Fuzheado), based on SDC routines from Botmultichill\n",
    "\n",
    "import pywikibot\n",
    "from pywikibot import pagegenerators\n",
    "from pywikibot.comms import http\n",
    "\n",
    "import json\n",
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "# Parameters specific to the institution\n",
    "PARAMS = {\n",
    "    'test_mode': False,\n",
    "    'category': u'Category:Japanese prints in the Metropolitan Museum of Art',\n",
    "    'recurse': False,\n",
    "    'search_string': r'source[ ]*=[ ]*http[s]*://www.metmuseum.org/art/collection/search/(\\d+)',\n",
    "    'pid':'P3634',\n",
    "    'pid_summary': u'importing Met Object ID from source info from Commons template',\n",
    "    'institution_pid':'P195',\n",
    "    'institution_qid':'Q160236',\n",
    "    'institution_summary': u'adding collection based on Met Object ID',\n",
    "    'http_retries': 3\n",
    "}\n",
    "\n",
    "# Counters for final report\n",
    "counter = {\n",
    "    'id': 0,\n",
    "    'institution': 0,\n",
    "    'id_skip': 0,\n",
    "    'institution_skip': 0,\n",
    "    'unmatched': 0\n",
    "}\n",
    "\n",
    "def addClaim(mediaid, pid, instring, summary='', claimtype='qid') -> int:\n",
    "    \"\"\"addClaim - add a Wikibase claim for a property/pid that is a Q object\n",
    "    \n",
    "    :param mediaid: MID of Commons file\n",
    "    :param pid: Wikidata property (eg. P195)\n",
    "    :param instring: either QID for an object (eg. Q160236) or a text string\n",
    "    :param summary: edit summary to append to automated summary\n",
    "    :param claimtype: should be 'qid' or 'string'\n",
    "    :return: return value\n",
    "    \"\"\"\n",
    "    pywikibot.output(u'Edit %s: %s->%s. %s' % (mediaid, pid, instring, summary))\n",
    "\n",
    "    # Check for existing entry - if it exists at all, skip and honor existing entry\n",
    "    request = site._simple_request(action='wbgetentities',ids=mediaid)\n",
    "    data = request.submit()\n",
    "    \n",
    "    # Check in case there are no SDC statements, or existing pid statement\n",
    "    try:\n",
    "        if (data.get(u'entities').get(mediaid).get(u'statements').get(pid)):\n",
    "            pywikibot.output(u'  Existing entry: skipping to be safe.')\n",
    "            return 0  # Skip\n",
    "    except AttributeError:\n",
    "        pass\n",
    "        \n",
    "    tokenrequest = http.fetch(u'https://commons.wikimedia.org/w/api.php?action=query&meta=tokens&type=csrf&format=json')\n",
    "\n",
    "    tokendata = json.loads(tokenrequest.text)\n",
    "    token = tokendata.get(u'query').get(u'tokens').get(u'csrftoken')\n",
    "    \n",
    "    # Determine claimtype\n",
    "    if claimtype == 'qid':\n",
    "        jstring = {\"entity-type\":\"item\",\"numeric-id\": instring.replace(u'Q', u'')}\n",
    "        postvalue = json.dumps(jstring)\n",
    "    elif claimtype == 'string':\n",
    "        postvalue = '\"'+instring+'\"'\n",
    "    else:\n",
    "        # Bad claimtype passed in, should never get here\n",
    "        pywikibot.output(u'  Error: improper claimtype passed %s' % (claimtype,))\n",
    "        return -1\n",
    "\n",
    "    postdata = {u'action' : u'wbcreateclaim',\n",
    "                u'format' : u'json',\n",
    "                u'entity' : mediaid,\n",
    "                u'property' : pid,\n",
    "                u'snaktype' : u'value',\n",
    "                u'value' : postvalue,\n",
    "                u'token' : token,\n",
    "                u'summary' : summary\n",
    "                }\n",
    "\n",
    "    if PARAMS['test_mode']:\n",
    "        pywikibot.output('TEST_MODE: addClaim: %s, %s' % (claimtype, postdata))\n",
    "        pass\n",
    "    else:\n",
    "        apipage = http.fetch(u'https://commons.wikimedia.org/w/api.php', method='POST', data=postdata)\n",
    "        if not apipage.ok:\n",
    "            pywikibot.output('  Error http status code %s: %s %s' % (apipage.status_code, claimtype, postdata))\n",
    "            return -2\n",
    "        \n",
    "    return 1  # Successful add\n",
    "\n",
    "\n",
    "def media_id(page: pywikibot.page.FilePage) -> str:\n",
    "    \"\"\"Return MID from page ID: just prepend an M to number\"\"\"\n",
    "    return u'M%s' % (page.pageid,)\n",
    "\n",
    "\n",
    "# Start execution\n",
    "\n",
    "# Set this manually if you want to override the default\n",
    "# PARAMS['category'] = u'Category:Bathing suits in the Metropolitan Museum of Art',\n",
    "\n",
    "# Shouldn't need to touch these\n",
    "site = pywikibot.Site(u'commons', u'commons')\n",
    "cat = pywikibot.Category(site,PARAMS['category'])\n",
    "gen = pagegenerators.CategorizedPageGenerator(cat, recurse=PARAMS['recurse'])\n",
    "patt = re.compile(PARAMS['search_string'])\n",
    "\n",
    "# Start processing items in category\n",
    "# for page in islice(gen, 10):\n",
    "for page in gen:\n",
    "\n",
    "    text = page.text  # Grab commons page text\n",
    "    mediaid = media_id(page) # Determine MID of form M23456\n",
    "\n",
    "    m = patt.search(text) # Find URL string that contains object number\n",
    "    if m:\n",
    "        id_string = m.group(1)  # Extract object ID from regex match group\n",
    "        summary = PARAMS['pid_summary']\n",
    "        result = addClaim(mediaid, PARAMS['pid'], id_string, summary, claimtype='string')\n",
    "\n",
    "        if result>0:\n",
    "            counter['id'] += 1\n",
    "        else:\n",
    "            counter['id_skip'] += 1\n",
    "\n",
    "        if 'institution_pid' not in PARAMS:\n",
    "            next\n",
    "            \n",
    "        institution = PARAMS['institution_qid']  # Met QID\n",
    "        summary = PARAMS['institution_summary']\n",
    "        result = addClaim(mediaid, PARAMS['institution_pid'], institution, summary, claimtype='qid')\n",
    "\n",
    "        if result>0:\n",
    "            counter['institution'] += 1\n",
    "        else:\n",
    "            counter['institution_skip'] += 1\n",
    "    else:\n",
    "        counter['unmatched'] += 1\n",
    "        pywikibot.output(u'Unmatched %s' % (mediaid, ))\n",
    "\n",
    "# Output report\n",
    "pywikibot.output ('Final report')\n",
    "pywikibot.output (PARAMS['category'])\n",
    "for x in counter:\n",
    "    pywikibot.output('%s: %s' % (x, counter[x]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
